{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据集上的一些问题\n",
    "\n",
    "数据集为**DBRHD**, 搜索数据集的时候大部分参考资料都指向数据集**Pen-Based Recognition of Handwritten Digit**;\n",
    "但是参考资料展示的结果都是**Optical Recognition of Handwritten Digits**, 本实验用的数据集是**Pen-Based Recognition of Handwritten Digit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "'''加载数据集'''\n",
    "\n",
    "# 初始化文件路径\n",
    "data_path = '.\\\\pen+based+recognition+of+handwritten+digits'\n",
    "train_files = os.path.join(data_path, 'pendigits.tra')\n",
    "test_files = os.path.join(data_path, 'pendigits.tes')\n",
    "\n",
    "# 加载特征和标签\n",
    "with open(train_files, 'r') as f:\n",
    "    train_datas = f.readlines()\n",
    "f.close()\n",
    "\n",
    "with open(test_files, 'r') as f:\n",
    "    test_datas = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# 将训练集和测试集的特征和标签分开\n",
    "train_X = []\n",
    "train_y = []\n",
    "test_X = []\n",
    "test_y = []\n",
    "for train_data in train_datas:\n",
    "    train_data = train_data.split(',')\n",
    "    X = [int(x) for x in train_data[:-1]]\n",
    "    y = int(train_data[-1])\n",
    "    train_X.append(X)\n",
    "    train_y.append(y)\n",
    "\n",
    "for test_data in test_datas:\n",
    "    test_data = test_data.split(',')\n",
    "    X = [int(x) for x in test_data[:-1]]\n",
    "    y = int(test_data[-1])\n",
    "    test_X.append(X)\n",
    "    test_y.append(y)\n",
    "\n",
    "print(f'数据集加载完毕，共有{len(train_X)}个训练样本, 共有{len(test_X)}个测试样本, 下面是一个训练数据的样例:')\n",
    "print(f'特征: {train_X[0]}\\n标签: {train_y[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 采用sklearn库中的函数, 实现基于KNN算法实现手写数字识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''使用sklearn完成基于knn的手写数字识别模型'''\n",
    "\n",
    "# 模型构建\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "def build_knn_classifier(k):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_classifier.fit(train_X, train_y)\n",
    "    return knn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试KNN\n",
    "def test_knn_classifier():\n",
    "    neighbors = [1,3,5,7]\n",
    "    wrong_sum = []\n",
    "    accuracy = []\n",
    "    for neighbor in neighbors:\n",
    "        # 构建模型\n",
    "        knn_classifier = build_knn_classifier(neighbor)\n",
    "        prediction = knn_classifier.predict(test_X)\n",
    "\n",
    "        # 保存错误个数和正确率\n",
    "        wrong = numpy.sum(prediction != test_y)\n",
    "        wrong_sum.append(wrong)\n",
    "        accuracy.append(1 - wrong / float(len(test_X)))\n",
    "\n",
    "    # 输出结果\n",
    "    res = {\n",
    "        '邻居数量K': neighbors,\n",
    "        '错误数量': wrong_sum,\n",
    "        '正确率': accuracy\n",
    "    }\n",
    "\n",
    "    output = pd.DataFrame(res)\n",
    "    print(output)\n",
    "\n",
    "test_knn_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 采用sklearn库中的神经网络函数, 实现基于全连接的神经网络模型的手写数字识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''使用sklearn完成基于MLP的手写数字识别模型'''\n",
    "\n",
    "# 模型构建\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def build_mlp_classifier(hid, lr):\n",
    "    mlp_classifier = MLPClassifier(hidden_layer_sizes=hid, learning_rate_init=lr,\n",
    "                                    activation='logistic',\n",
    "                                    solver='adam',\n",
    "                                    max_iter=10, batch_size=256)\n",
    "    mlp_classifier.fit(train_X, train_y)\n",
    "    return mlp_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试MLP\n",
    "def test_mlp_classifier():\n",
    "    hid_list = [500, 1000, 1500, 2000]\n",
    "    lr_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "    wrong_sum_hid = []\n",
    "    accuracy_hid = []\n",
    "    wrong_sum_lr = []\n",
    "    accuracy_lr = []\n",
    "    for hid in hid_list:\n",
    "        # 构建模型\n",
    "        mlp_classifier = build_mlp_classifier(hid, 1e-4)\n",
    "        prediction = mlp_classifier.predict(test_X)\n",
    "\n",
    "        # 保存错误个数和正确率\n",
    "        wrong = numpy.sum(prediction != test_y)\n",
    "        wrong_sum_hid.append(wrong)\n",
    "        accuracy_hid.append(1 - wrong / float(len(test_X)))\n",
    "\n",
    "    for lr in lr_list:\n",
    "        # 构建模型\n",
    "        mlp_classifier = build_mlp_classifier(1000, lr)\n",
    "        prediction = mlp_classifier.predict(test_X)\n",
    "\n",
    "        # 保存错误个数和正确率\n",
    "        wrong = numpy.sum(prediction != test_y)\n",
    "        wrong_sum_lr.append(wrong)\n",
    "        accuracy_lr.append(1 - wrong / float(len(test_X)))\n",
    "\n",
    "    # 输出结果\n",
    "    res_hid = {\n",
    "        '隐层节点数': hid_list,\n",
    "        '错误数量': wrong_sum_hid,\n",
    "        '正确率': accuracy_hid\n",
    "    }\n",
    "\n",
    "    res_lr = {\n",
    "        '学习率': lr_list,\n",
    "        '错误数量': wrong_sum_lr,\n",
    "        '正确率': accuracy_lr\n",
    "    }\n",
    "\n",
    "    output_hid = pd.DataFrame(res_hid)\n",
    "    output_lr = pd.DataFrame(res_lr)\n",
    "    print(output_hid)\n",
    "    print(output_lr)\n",
    "\n",
    "test_mlp_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 采用Numpy库实现全连接的神经网络模型, 实现基于全连接的神经网络的手写数字识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "def dataloader(batch_size):\n",
    "    # 初始化文件路径\n",
    "    data_path = '.\\\\pen+based+recognition+of+handwritten+digits'\n",
    "    train_files = os.path.join(data_path, 'pendigits.tra')\n",
    "    test_files = os.path.join(data_path, 'pendigits.tes')\n",
    "\n",
    "    # 加载特征和标签\n",
    "    with open(train_files, 'r') as f:\n",
    "        train_datas = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    with open(test_files, 'r') as f:\n",
    "        test_datas = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # 将训练集和测试集的特征和标签分开\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    test_X = []\n",
    "    test_y = []\n",
    "    for train_data in train_datas:\n",
    "        train_data = train_data.split(',')\n",
    "        X = [int(x) for x in train_data[:-1]]\n",
    "        y = int(train_data[-1])\n",
    "        train_X.append(X)\n",
    "        train_y.append(y)\n",
    "\n",
    "    for test_data in test_datas:\n",
    "        test_data = test_data.split(',')\n",
    "        X = [int(x) for x in test_data[:-1]]\n",
    "        y = int(test_data[-1])\n",
    "        test_X.append(X)\n",
    "        test_y.append(y)\n",
    "    \n",
    "    len_train_data = len(train_X)\n",
    "    len_test_data = len(test_X)\n",
    "    train_X = np.array(train_X)\n",
    "    train_y = np.array(train_y)\n",
    "    test_X = np.array(test_X)\n",
    "    test_y = np.array(test_y)\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    for i in range(len(train_X)):\n",
    "        train_dataset.append((train_X[i], train_y[i]))\n",
    "    for i in range(len(test_X)):\n",
    "        test_dataset.append((test_X[i], test_y[i]))\n",
    "    train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = data.DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "    return train_dataloader, test_dataloader, len_train_data, len_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, n_in, n_out, batch_size, lr=0.001):\n",
    "        self.W = np.random.normal(scale=0.01, size=(n_in, n_out))\n",
    "        self.b = np.zeros((batch_size, n_out))\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        output = np.dot(x, self.W) + self.b\n",
    "        output = np.dot(x, self.W)\n",
    "        # 以sigmoid作为激活函数\n",
    "        output = 1 / (1 + np.exp(-output))\n",
    "        self.activated_output = output\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.activated_output * (1 - self.activated_output) * dout\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = dout\n",
    "        self.W = self.W - self.dW * self.lr / self.batch_size\n",
    "        self.b = self.b - self.db * self.lr / self.batch_size\n",
    "        return dx\n",
    "    \n",
    "class SoftMax:\n",
    "    y_hat = []\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SoftMax, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_exp = np.exp(x)\n",
    "        partition = np.sum(x_exp, axis=1, keepdims=True)\n",
    "        self.y_hat = x_exp / partition\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, y):\n",
    "        dout = self.y_hat - y\n",
    "        return dout\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, input_size, batch_size, num_classes, lr=0.001, hidden_layer_sizes=(256,)):\n",
    "\n",
    "        self.layer_list = [[hidden_layer_sizes[i], hidden_layer_sizes[i + 1]] for i in range(len(hidden_layer_sizes) - 1)]\n",
    "        self.input_layer = LinearLayer(input_size, hidden_layer_sizes[0], batch_size, lr=lr)\n",
    "        self.out_layer = LinearLayer(hidden_layer_sizes[-1], num_classes, batch_size, lr=lr)\n",
    "        self.softmax = SoftMax()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.layers = [self.input_layer]\n",
    "        for i in range(len(self.layer_list)):\n",
    "            self.layers.append(LinearLayer(self.layer_list[i][0], self.layer_list[i][1], batch_size, lr=lr))\n",
    "        self.layers.append(self.out_layer)\n",
    "        self.layers.append(self.softmax)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_mlp_numpy_classifier(hid, lr):\n",
    "    num_epochs = 10\n",
    "    batch_size = 6\n",
    "\n",
    "    train_dataloader, test_dataloader, len_train_data, len_test_data = dataloader(batch_size)\n",
    "\n",
    "    model = MLP(input_size=16, batch_size=batch_size, num_classes=10, lr=lr, hidden_layer_sizes=(hid,))\n",
    "    # model.parameter()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_accuracy = 0\n",
    "        test_accuracy = 0\n",
    "        test_wrong_num = 0\n",
    "        \n",
    "        # 这里确保batch_size能同时整除train_data和test_data\n",
    "        with tqdm(train_dataloader, unit='batch') as train_epoch:\n",
    "            for data, label in train_epoch:\n",
    "                train_epoch.set_description(f\"Epoch {epoch} Training\")\n",
    "                data = data.numpy()\n",
    "                label = label.numpy()\n",
    "                outputs = model.forward(data)\n",
    "                train_accuracy += (outputs.argmax(1) == label).sum() / len_train_data * 100\n",
    "                model.backward(np.eye(10)[label])\n",
    "        with tqdm(test_dataloader, unit='batch') as test_epoch:\n",
    "            for data, label in test_epoch:\n",
    "                test_epoch.set_description(f\"Epoch {epoch} Testing\")\n",
    "                data = data.numpy()\n",
    "                label = label.numpy()\n",
    "                outputs = model.forward(data)\n",
    "                test_accuracy += (outputs.argmax(1) == label).sum() / len_test_data * 100\n",
    "                test_wrong_num += (outputs.argmax(1) != label).sum()\n",
    "\n",
    "        print(f'******\\nEpoch {epoch} 结果如下:\\n训练准确率 {train_accuracy:.4f}%\\n测试错误数量 {test_wrong_num}, 测试准确率 {test_accuracy:.4f}%\\n******')\n",
    "\n",
    "hid_list = [500, 1000, 1500, 2000]\n",
    "lr_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for hid in hid_list:\n",
    "    print(f'******当前隐层节点数为 {hid}, 学习率为 {1e-1}, 训练开始******')\n",
    "    test_mlp_numpy_classifier(hid, 1e-1)\n",
    "    print(f'******当前隐层节点数为 {hid}, 学习率为 {1e-1}, 训练结束******\\n')\n",
    "\n",
    "for lr in lr_list:\n",
    "    print(f'******当前隐层节点数为 {1000}, 学习率为 {lr}, 训练开始******')\n",
    "    test_mlp_numpy_classifier(1000, 1e-1)\n",
    "    print(f'******当前隐层节点数为 {1000}, 学习率为 {lr}, 训练结束******\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 采用Pytorch库中的神经网络函数, 实现基于CNN的手写数字识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "class DBRHD_Dataset(data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        super(DBRHD_Dataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        with open(self.data_path, 'r') as f:\n",
    "            self.datas = f.readlines()\n",
    "        f.close()\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for data in self.datas:\n",
    "            data = data.split(',')\n",
    "            X = [[int(x)] for x in data[:-1]]\n",
    "            y = [int(data[-1])]\n",
    "            self.X.append(X)\n",
    "            self.y.append(y)\n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs={}\n",
    "        data = torch.from_numpy(self.X[index]).float()\n",
    "        labels = torch.from_numpy(self.y[index])\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "class CNN_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Classifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "def test_cnn_classifier(lr):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 256\n",
    "    lr = lr\n",
    "\n",
    "    train_dataset = DBRHD_Dataset(train_files)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size)\n",
    "    test_dataset = DBRHD_Dataset(test_files)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size)\n",
    "\n",
    "    cnn_classifier = CNN_Classifier()\n",
    "    cnn_classifier.to(device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn_classifier.parameters(), lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_correct, train_loss = 0, 0\n",
    "        test_correct, test_wrong_num = 0, 0\n",
    "        cnn_classifier.train()\n",
    "        with tqdm(train_dataloader, unit=\"batch\") as train_epoch:\n",
    "            for batch_idx, (data, labels) in enumerate(train_epoch):\n",
    "                train_epoch.set_description(f\"Epoch {epoch} Training\")\n",
    "\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = cnn_classifier(data)\n",
    "                labels = labels.squeeze(1)\n",
    "                labels = labels.long()\n",
    "                loss = loss_func(outputs, labels)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                predictions = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                        \n",
    "                train_loss += loss\n",
    "                train_correct += torch.sum(predictions == labels)\n",
    "\n",
    "        cnn_classifier.eval()\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as test_epoch:\n",
    "            for batch_idx, (data, labels) in enumerate(test_epoch):\n",
    "                test_epoch.set_description(f\"Epoch {epoch} Testing\")\n",
    "\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = cnn_classifier(data)\n",
    "                predictions = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "                test_correct += torch.sum(predictions == labels)\n",
    "                test_wrong_num += torch.sum(predictions != labels)\n",
    "\n",
    "        train_accuracy = train_correct / (batch_size * len(train_dataloader)) * 100\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        test_accuracy = test_correct / (batch_size * len(test_dataloader)) * 100\n",
    "        test_wrong_num = test_wrong_num\n",
    "\n",
    "        print(f'******\\nEpoch {epoch} 结果如下:\\n训练损失 {train_loss:.4f} , 训练准确率 {train_accuracy:.4f}%\\n测试错误数量 {test_wrong_num}, 测试准确率 {test_accuracy:.4f}%\\n******')\n",
    "\n",
    "lr_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for lr in lr_list:\n",
    "    print(f'******当前学习率为 {lr}, 训练开始******')\n",
    "    test_cnn_classifier(lr)\n",
    "    print(f'******当前学习率为 {lr}, 训练结束******\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distdepth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
